{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6b187d-113d-4933-a88c-d8c51bbe5e2e",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "416e132a-717f-46ee-842b-6fd56deca06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b830d9e6-f781-4b70-9937-6425daa8aa25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ank/workspace/hub/leoank/chirpminds/chirpminds/notes')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\".\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d856be5-d699-402c-a755-240089d61054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\".\").resolve().parents[1].joinpath(\"scratch/dataset/samples/sample_001.mp4\").is_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d4faf-cb5b-4fd6-bf5c-dd05580a5ce2",
   "metadata": {},
   "source": [
    "# Extract frames from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c4224-1b74-4080-b7ff-7947fa493c44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from subprocess import run as srun\n",
    "import shelx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b7bd9-f19e-4f62-8abb-eaa257e29bec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_frames(in_video: Path, out_dir: Path) -> None:\n",
    "    cmd = f\"ffmpeg -i {in_video.resolve()} -r 2 -s 640x360 -q:v 2 {out_dir.resolve()}/frame%d.jpeg\"\n",
    "    cmd = shelx.splilt(cmd)\n",
    "    srun(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c6246-cd51-4b96-b1a9-068e852131f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_files(vid_list: list[Path], out_dir: Path) -> None:\n",
    "    for vid in vid_list:\n",
    "        extract_frames(vid, out_dir.joinpath(vid.stem))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e9a1ec-22bf-4322-baf4-fc2964522dc4",
   "metadata": {},
   "source": [
    "# Automatic annotation with Grounded SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428ade3-264e-4476-afa6-bf385fdd51e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_grounded_sam_2 import GroundedSAM2\n",
    "from autodistill.detection import CaptionOntology\n",
    "\n",
    "# define an ontology to map class names to our GroundedSAM prompt\n",
    "# the ontology dictionary has the format {caption: class}\n",
    "# where caption is the prompt sent to the base model, and class is the label that will\n",
    "# be saved for that caption in the generated annotations\n",
    "# then, load the model\n",
    "base_model = GroundedSAM2(\n",
    "    ontology=CaptionOntology(\n",
    "        {\n",
    "            \"chickadee\": \"chickadee\",\n",
    "            \"cage\": \"cage\",\n",
    "            \"leaves\": \"leaves\"\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# label all images in a folder called `context_images`\n",
    "dataset = base_model.label(\"./images\", extension=\".png\", output_folder=\"./labeled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ad8b90-5db4-4ddf-8c89-d22d509e514c",
   "metadata": {},
   "source": [
    "# Manual annotation refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a8b7d-e236-4156-8675-aca82796516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imjoy_rpc.hypha.sync import connect_to_server\n",
    "import asyncio\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# %%\n",
    "PORT = \"9191\"\n",
    "\n",
    "SERVER_CONFIG = {\n",
    "  \"name\": \"piximi-server\",\n",
    "  \"server_url\": f\"http://localhost:{PORT}\",\n",
    "  \"config\": {\n",
    "      \"visibility\": \"private\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# %%\n",
    "piximi_server = connect_to_server(SERVER_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b94e8-12b7-4b32-aeb8-8f0dafbfd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "piximi_server.getConnectionInfo()\n",
    "\n",
    "# %%\n",
    "token = piximi_server.generate_token()\n",
    "workspace = piximi_server.getWorkspaceInfo()\n",
    "\n",
    "# %%\n",
    "print(\"Name:\", piximi_server.name)\n",
    "print(\"workspace\", workspace)\n",
    "print(\"token\", token)\n",
    "\n",
    "# %%\n",
    "PIXIMI_URL = \"http://localhost:3000\"\n",
    "connection_url = f\"{PIXIMI_URL}?hypha_token={token}&hypha_workspace={workspace['name']}\"\n",
    "\n",
    "# %%\n",
    "# !echo -n \"$connection_url\" | pbcopy\n",
    "\n",
    "# %%\n",
    "# !pbpaste\n",
    "\n",
    "# %% [markdown]\n",
    "# In Piximi go to piximi.app followed by the query paramater above, wait for load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1717ef1b-8536-4e33-aa35-8f1006c8f745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "im = io.imread(\"/Users/Nodar/Developer/piximi/piximi/src/images/8b_2c_actin-nuclei.tif\", plugin=\"pil\")\n",
    "\n",
    "# %%\n",
    "io.available_plugins\n",
    "\n",
    "# %%\n",
    "im.shape, im.dtype\n",
    "\n",
    "# %%\n",
    "plt.imshow(np.stack([(im[0]/im[0].max()*255).astype(\"uint8\"), (im[1]/47*255).astype(\"uint8\"), np.zeros([im.shape[1], im.shape[2]]).astype(\"uint8\")]).transpose([1,2,0]))\n",
    "\n",
    "# %%\n",
    "piximi_receive_svc = piximi_server.get_service(\"piximi-annotator-receiver\")\n",
    "\n",
    "piximi_receive_svc\n",
    "\n",
    "# %%\n",
    "piximi_receive_svc.receiveImage(im, \"hypha_image\")\n",
    "\n",
    "# %%\n",
    "received_annotations = []\n",
    "\n",
    "\n",
    "# %%\n",
    "def send_annotations(annotations):\n",
    "    print(\"Received annotations\")\n",
    "    received_annotations.append(annotations)\n",
    "\n",
    "\n",
    "# %%\n",
    "piximi_send_svc = piximi_server.register_service({\n",
    "    \"name\": \"PIXIMI Annotator send service\",\n",
    "    \"id\": \"piximi-annotator-sender\",\n",
    "    \"send_annotations\": send_annotations\n",
    "})\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# In Piximi, add annotations to the image, click back, and click save\n",
    "\n",
    "# %%\n",
    "len(received_annotations)\n",
    "\n",
    "# %%\n",
    "type(received_annotations[0])\n",
    "\n",
    "# %%\n",
    "len(received_annotations[0])/im.shape[1]/im.shape[2]\n",
    "\n",
    "# %%\n",
    "x = [d for d in received_annotations[0]]\n",
    "\n",
    "# %%\n",
    "xx = np.asarray(x).reshape((im.shape[1], im.shape[2], 4))\n",
    "\n",
    "# %%\n",
    "plt.imshow(xx)\n",
    "\n",
    "# %%\n",
    "plt.imshow(xx[:,:,0] * 127)\n",
    "\n",
    "# %%\n",
    "# async def main():\n",
    "#     server = await connect_to_server({\"server_url\": \"http://localhost:9000\"})\n",
    "\n",
    "#     # Get an existing service\n",
    "#     # Since \"hello-world\" is registered as a public service, we can access it using only the name \"hello-world\"\n",
    "#     svc = await server.get_service(\"hello-world\")\n",
    "#     ret = await svc.hello(\"John\")\n",
    "#     print(ret)\n",
    "\n",
    "# asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fa3049-a19f-40d4-bcee-fb9890fbd649",
   "metadata": {},
   "source": [
    "# Train object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4584410-9fc8-4f88-9482-079fa7a2ee21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autodistill_yolonas import YOLONAS\n",
    "\n",
    "target_model = YOLONAS(\"\")\n",
    "target_model.train(\"./labeled/data.yaml\", epochs=200, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9971c2-2626-41af-87fd-cfd4f6ca154a",
   "metadata": {},
   "source": [
    "# Use trained model for object detection and perform tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87310dd9-11bd-483d-ac39-81ef5bcb26f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "segmodel = YOLO(\"../../runs/segment/train2/weights/best.pt\")\n",
    "tracker = sv.ByteTrack(lost_track_buffer=300)\n",
    "\n",
    "bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "mask_annotator = sv.MaskAnnotator()\n",
    "label_annotator = sv.LabelAnnotator()\n",
    "trace_annotator = sv.TraceAnnotator(trace_length=500)\n",
    "\n",
    "\n",
    "def callback(frame: np.ndarray, index: int) -> np.ndarray:\n",
    "    # results = model(frame)[0]\n",
    "    resultseg = segmodel(frame)[0]\n",
    "    segdet = sv.Detections.from_ultralytics(resultseg)\n",
    "    # dsegdettections = sv.Detections.from_ultralytics(results)\n",
    "    detections = segdet\n",
    "    # print(f\"Masks: {(detections.mask.size)}\")\n",
    "    detections = tracker.update_with_detections(detections)\n",
    "\n",
    "    labels = [f\"#{tracker_id}\" for tracker_id in detections.tracker_id]\n",
    "\n",
    "    annotated_frame = mask_annotator.annotate(scene=frame.copy(), detections=segdet)\n",
    "    annotated_frame = bounding_box_annotator.annotate(\n",
    "        scene=frame.copy(), detections=detections\n",
    "    )\n",
    "    annotated_frame = label_annotator.annotate(\n",
    "        scene=annotated_frame, detections=detections, labels=labels\n",
    "    )\n",
    "    annotated_frame = trace_annotator.annotate(\n",
    "        scene=annotated_frame, detections=detections\n",
    "    )\n",
    "    return annotated_frame\n",
    "\n",
    "\n",
    "sv.process_video(\n",
    "    source_path=\"samples/sample_001.mp4\",\n",
    "    target_path=\"results/sample_003.mp4\",\n",
    "    callback=callback,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43a62e0-8ae9-4af1-8077-96420ee9a25e",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ef9cd3-2051-4f38-a880-b1e8a5251a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"../../scratch/dataset/samples/sample_001.mp4\" controls  width=\"1024\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "Video(\"../../scratch/dataset/samples/sample_001.mp4\", width=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5754470c-d8e5-470c-b461-5637a8e4c379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
